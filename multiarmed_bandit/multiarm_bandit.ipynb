{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "multiarm_bandit.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mlarionov/presentations/blob/master/multiarmed_bandit/multiarm_bandit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFAKLy9PGfp-",
        "colab_type": "text"
      },
      "source": [
        "# Multi-Arm Bandit Algorithm\n",
        "\n",
        "## References\n",
        "1. *Practical Statistics for Data Scientists: 50 Essential Concepts* by Peter Bruce, Andrew Bruce\n",
        "1. https://en.wikipedia.org/wiki/Multi-armed_bandit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LXjmPCVGfp_",
        "colab_type": "text"
      },
      "source": [
        "## Problem statement\n",
        "**Multi-arm bandit**\n",
        "An imaginary slot machine with multiple arms for the customer to choose from, each with different payoffs, here taken to be an analogy for a multitreatment experiment.\n",
        "\n",
        "Algorithm is popular A/B testing.\n",
        "\n",
        "Each arm has an unknown probability of win or, in general, expected payoff.\n",
        "You have only limited number of tries.\n",
        "Your goal is to maximize the payoff."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RroEER4GfqA",
        "colab_type": "text"
      },
      "source": [
        "## Exploration vs. Exploitation\n",
        "\n",
        "Exploration: you spend the first several rounds randomly selecting \"arms\", then pick the best one and use only that one. But you potentially wasted these rounds by not choosing the best earlier.\n",
        "\n",
        "Exploitation: you try to figure out quickly which is the best arm and use only it. But you may have picked the wrong one. In that case you should have spent more time exploring."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mcdo7axXGfqB",
        "colab_type": "text"
      },
      "source": [
        "## Bandit algorithms\n",
        "\n",
        "Hybrid approach. Keep exploring, but give preference the \"arm\" with the best results at that moment.\n",
        "\n",
        "### Epsilon-greedy algorithm\n",
        "\n",
        "1. Generate a random number between 0 and 1. \n",
        "1. If the number lies between 0 and epsilon (where epsilon is a number between 0 and 1, typically fairly small), pick the \"arm\" at random\n",
        "3. If the number is â‰¥ epsilon, show whichever offer has had the highest reward to date.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJFpWLASGfqC",
        "colab_type": "text"
      },
      "source": [
        "### Thompson Sampling\n",
        "\n",
        "After each step we reevaluate expected reward for each \"arm\", then pick the \"arm\" with the best expected reward. For example, suppose you can win `$`1, `$`5 or `$`10. Then if the respective probabilities are $p_1$, $p_5$ and $p_10$, the expected redward is:\n",
        "\n",
        "$$ E[reward] = p_1 + 5 p_5 + 10 p_10 $$\n",
        "\n",
        "Now, rather than use the point estimates of the probabilities $p_i$, we take a sample from the posterior distribution. \n",
        "\n",
        "Just to remind you the Bayes rule:\n",
        "\n",
        "$$ p(A|B) = \\frac{p(B|A) p(A)}{p(B)} $$\n",
        "\n",
        "The probabilities of the rewards follow the Multinomial distribution (or Binomial distribution in case we can have only a reward of fixed value). The probability that we receive reward #1 $n_1$ times, reward #2 $n_2$ times, etc. is:\n",
        "\n",
        "$$ p(n_1 ... n_k) = \\frac{n!}{n_1! ... n_k!} q_1^{n_1} ... q_k^{n_k}$$\n",
        "\n",
        "Here  $\\sum_{i=1}^k{ q_i }= 1$.\n",
        "\n",
        "The prior distribution of the parameters $q$ is Dirichlet distribution:\n",
        "\n",
        "$$ p(q) = \\frac{1}{B(\\alpha)}  \\prod_{i=1}^k {q_i^{\\alpha_i-1}} $$\n",
        "\n",
        "Since we don't know anything about the multi-armed bandit until we started interacting with it, it makes sense to set all $\\alpha$ to the same value, for example, to 1.\n",
        "\n",
        "Since the Dirichlet distribution is a conjugate prior of the Multinomial distribution, the posterior distribution is also Dirichlet distribution with parameters $\\alpha_i = n_i$\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clBYeHg7GfqD",
        "colab_type": "text"
      },
      "source": [
        "So the algorithm is as follows:\n",
        "1. For each arm recalculate the parameters of the posterior distribution\n",
        "2. For each arm draw a sample from the posterior distribution\n",
        "3. For each arm calculate the expected reward\n",
        "3. Pick the arm that has the highest expected reward"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kbw5knIWzZD9",
        "colab_type": "text"
      },
      "source": [
        "As the parameters of the posterior distribution increase, the distribution becomes more and more narrow, and the exploration elements naturally dicreases in favor of the exploitation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-5yMH8oGfqE",
        "colab_type": "text"
      },
      "source": [
        "In our examples we will use a fixed reward, so instead of Multinomial distribution we will be using Binomial distribution, and instead of Dirichlet distribution we will use beta distribution. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "na47v7-HCTQL",
        "colab_type": "text"
      },
      "source": [
        "## Naive approach"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jj0oTOJQGfqF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "def draw(p):\n",
        "    return 1 if random.random() <= p else 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBCxUhKSGfqI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "random.seed(6)\n",
        "p_a = 0.5\n",
        "p_b = 0.51"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ND2pR5cl0M7d",
        "colab_type": "text"
      },
      "source": [
        "First we illustrate that the naive approach \"exploration first, exploitation second\" can be not optimal, especially when the data generating processes are close."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dmsYUKLrGfqL",
        "colab_type": "code",
        "outputId": "8f65bfde-f0ce-442a-ab64-2648fd9bf6b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "for i in range(10):\n",
        "    print('A:',draw(p_a), 'B:', draw(p_b))    "
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A: 0 B: 0\n",
            "A: 1 B: 1\n",
            "A: 1 B: 0\n",
            "A: 1 B: 0\n",
            "A: 1 B: 0\n",
            "A: 1 B: 0\n",
            "A: 0 B: 1\n",
            "A: 0 B: 0\n",
            "A: 1 B: 0\n",
            "A: 0 B: 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzAndDb2GfqO",
        "colab_type": "text"
      },
      "source": [
        "As you see, even though the probability of the second arm is higher, for the first 10 trials we got better reward for a weaker arm.\n",
        "\n",
        "\n",
        "Next we will try epsilon-greedy strategy, and will try to see what is the best value of the epsilon."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WY9Aw2UFCkxq",
        "colab_type": "text"
      },
      "source": [
        "## Epsilon-greedy algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AY-fkfFkGfqP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Arm(object):\n",
        "  successes = 0\n",
        "  failures = 0\n",
        "  \n",
        "  def __init__(self, p):\n",
        "    self.p = p\n",
        "  \n",
        "  def prob(self):\n",
        "    if self.successes + self.failures == 0:\n",
        "      return 0.5\n",
        "    else:\n",
        "      return self.successes / (self.successes + self.failures)\n",
        "    \n",
        "  def add_success(self):\n",
        "    self.successes += 1\n",
        "    \n",
        "  def add_failure(self):\n",
        "    self.failures += 1\n",
        "    \n",
        "  def attempts(self):\n",
        "    return self.successes + self.failures\n",
        "   \n",
        "  \n",
        "  \n",
        "def epsilon_greedy(p_a, p_b, epsilon, max_attempts):\n",
        "    a = Arm(p_a)\n",
        "    b = Arm(p_b)\n",
        "    \n",
        "    for _ in range(max_attempts):\n",
        "      if draw(epsilon) or a.prob() == b.prob():\n",
        "        selected = random.choices((a, b))[0]\n",
        "      else:\n",
        "        selected = a if a.prob() > b.prob() else b\n",
        "    \n",
        "      if draw(selected.p):\n",
        "        selected.add_success()\n",
        "      else:\n",
        "        selected.add_failure()\n",
        "    \n",
        "    return a.successes + b.successes, a.attempts() ,b.attempts()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PXWLSO2KGfqR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b5bbe9ed-6852-4946-e635-4cbd131e6d9f"
      },
      "source": [
        "p_a = 0.5\n",
        "p_b = 0.6\n",
        "random.seed(5)\n",
        "print(epsilon_greedy(p_a, p_b, 0, 1000))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(519, 999, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-KpZqAgNB7S",
        "colab_type": "text"
      },
      "source": [
        "Not great. But look, since the epsilon is zero, it sticks to its first choice, which in our case is the worse arm. Let's try to increase epsilon to see if we can get better results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_VmQtg1LxJ0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "653bc645-0b30-4f41-dd68-ea695d75270f"
      },
      "source": [
        "random.seed(5)\n",
        "print(epsilon_greedy(p_a, p_b, 0.3, 1000))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(583, 277, 723)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rnDrrn_oN2sJ",
        "colab_type": "text"
      },
      "source": [
        "This is better! Not only we've got more money, but we also have the algorithm prefer the second arm. This means that the algorithm has learned which arm is better. Let's see the results we get for different values of epsilon:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gfs5yDyDM6ey",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from statistics import mean\n",
        "eps = []\n",
        "results = []\n",
        "for epsilon in np.linspace(start=0.0, stop=1.0, num=40):\n",
        "  attempts = []\n",
        "  for seed in range(100):\n",
        "    random.seed(seed)\n",
        "    attempts.append(epsilon_greedy(p_a, p_b, epsilon, 1000)[0])\n",
        "  eps.append(epsilon)\n",
        "  results.append(mean(attempts))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJLOkGprQSji",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "ff9ec9e7-0881-4f94-a5ba-bff01666cd8d"
      },
      "source": [
        "%matplotlib inline\n",
        "from matplotlib import pyplot as plt\n",
        "plt.scatter(x=eps, y=results);"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yV7kT09ER617",
        "colab_type": "text"
      },
      "source": [
        "This seems like the performance generally worsense as epsilon increases. Let's try to see the best value of $\\epsilon$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04BTrrvrSRvh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "690ac662-309a-4b8b-a596-eb50d044c2b5"
      },
      "source": [
        "best_epsilon = eps[np.argmax(results)]\n",
        "best_epsilon"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.15384615384615385"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMAuK0EXT_RG",
        "colab_type": "text"
      },
      "source": [
        "And the best average reward is:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bFczmbfvUD9d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4c43bc80-a9cc-4b74-8cff-178c19cecfa8"
      },
      "source": [
        "best_reward = np.max(results)\n",
        "best_reward"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "586.28"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVDHg4vbBwY9",
        "colab_type": "text"
      },
      "source": [
        "## Thompson sampling. \n",
        "\n",
        "Since each arm has a fixed reward, its distribution can be modeled as Bernoulli distribution, and the corersponding conjugate prior is Beta distribution. It has two parameters $\\alpha$ and $\\beta$:\n",
        "\n",
        "$$P(q) = \\frac{q^{\\alpha-1}(1-q)^{\\beta-1}}{B(\\alpha,\\beta)} $$\n",
        "\n",
        "The uninformative prior correesponds to both $\\alpha$ and $\\beta$ equals to 1. The parameters of the poeterior distribution are updated using this simple rule:\n",
        "\n",
        "$$\\alpha = \\alpha +\\textrm{number of successes}$$\n",
        "$$\\beta = \\beta + \\textrm{number of failures}$$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "noCtAA16GRfc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ThompsonArm(Arm):\n",
        "  init_alpha = 1\n",
        "  init_beta = 1\n",
        "  \n",
        "  def sample(self):\n",
        "    alpha = self.init_alpha + self.successes\n",
        "    beta = self.init_beta + self.failures\n",
        "    return np.random.beta(alpha, beta)\n",
        "  \n",
        "def thompson(p_a, p_b, max_attempts):\n",
        "    a = ThompsonArm(p_a)\n",
        "    b = ThompsonArm(p_b)\n",
        "    \n",
        "    for _ in range(max_attempts):\n",
        "      a_sample = a.sample()\n",
        "      b_sample = b.sample()\n",
        "      if a_sample == b_sample:\n",
        "        selected = random.choices((a, b))[0]\n",
        "      else:\n",
        "        selected = a if a_sample > b_sample else b\n",
        "    \n",
        "      if draw(selected.p):\n",
        "        selected.add_success()\n",
        "      else:\n",
        "        selected.add_failure()\n",
        "    \n",
        "    return a.successes + b.successes, a.attempts() ,b.attempts()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFlqFaxxHVdl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "132716d4-c8ab-447e-98d1-bda41373b9c4"
      },
      "source": [
        "thompson(p_a, p_b, 1000)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(604, 130, 870)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urXgujHsI1g1",
        "colab_type": "text"
      },
      "source": [
        "We can see that it favors heavily the second arm. Let's run it multiple time and see what would be the average reward"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hh1pqyDLIiJ0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0b2e8500-db73-4c24-c467-1f7a97769388"
      },
      "source": [
        "for seed in range(1000):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    attempts.append(thompson(p_a, p_b, 1000)[0])\n",
        "print(np.mean(attempts))                   "
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "586.6348148148148\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSkaAPgkKqaS",
        "colab_type": "text"
      },
      "source": [
        "It is almost exactly the same as for the best reward of the epsilon-greedy algorithm, but the difference here is that we don't need to do hyperparameter tuning. Thompson sampling will almost always perform better than epsilon-greedy approach, when we don't have a luxury to try different values of $\\epsilon$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eB_H3-lLLoRx",
        "colab_type": "text"
      },
      "source": [
        "## Further research\n",
        "\n",
        "**Contextual bandit** algorithm is when you also have an input vector X as an additional help inmaking the right selection. Many algorithms exist in training the model, starting from linear models and going all the way up to multilayer neural networks.[link text](https://)"
      ]
    }
  ]
}